{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Goal\n",
    "\n",
    "## What're we doing?\n",
    "We're going to let XGBoost, LightGBM and Catboost battle it out in 3 rounds:\n",
    "\n",
    "- **Classification:** Classify images in the Fashion MNIST (60,000 rows, 784 features)\n",
    "\n",
    "- **Regression:** Predict NYC Taxi fares (60,000 rows, 7 features)\n",
    "\n",
    "- **Massive Dataset:** Predict NYC Taxi fares (2 million rows, 7 features)\n",
    "\n",
    "\n",
    "## How're we doing it?\n",
    "In each round here are the steps we'll follow:\n",
    "1. Train baseline models of XGBoost, Catboost, LightGBM (trained using the same paramaters for each model)\n",
    "2. Train fine-tuned models of XGBoost, Catboost, LightGBM using GridSearchCV\n",
    "3. Measure performance on the following metrics:\n",
    "    - training and prediction times\n",
    "    - prediction score\n",
    "    - interpretability (feature importance, shap values, visualize trees)\n",
    "\n",
    "## What does this all mean?\n",
    "A detailed analysis of the results can be found on my blog at: https://lavanya.ai/2019/06/27/battle-of-the-boosting-algorithms/\n",
    "\n",
    "I hope you find this analysis useful, I would love to hear your feedback on improving it! I encourage you to fork this kernel, and play with the code.\n",
    "\n",
    "If you like this kernel, please give it an upvote. Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Plots\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "figure(num=None, figsize=(20, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "import seaborn as sns\n",
    "from matplotlib.pylab import rcParams\n",
    "##set up the parameters\n",
    "rcParams['figure.figsize'] = 80,60\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from lightgbm.plotting import plot_importance\n",
    "import lightgbm\n",
    "import xgboost as xgb\n",
    "import catboost\n",
    "from xgboost import plot_tree\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "\n",
    "# Misc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dateutil import tz\n",
    "from geopy import distance\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Ignore useless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "pd.options.display.max_seq_items = 8000\n",
    "pd.options.display.max_rows = 8000\n",
    "\n",
    "print(os.listdir(\"../input/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_time(diff):\n",
    "   m, s = divmod(diff, 60)\n",
    "   h, m = divmod(m, 60)\n",
    "   s,m,h = int(round(s, 0)), int(round(m, 0)), int(round(h, 0))\n",
    "   print(\"Execution Time: \" + \"{0:02d}:{1:02d}:{2:02d}\".format(h, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in a classifier, calculates the training + prediction times and accuracy score, returns a model\n",
    "def Train(clf, X, y, X_predict, y_predict, type='classification'):\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    model = clf.fit(X,y)\n",
    "    end = time.time()\n",
    "    print('Training time: ')\n",
    "    show_time(end - start)\n",
    "    training_times.append(end - start)\n",
    "\n",
    "    # Predict\n",
    "    start = time.time()\n",
    "    if(type=='classification'):\n",
    "        scores.append(accuracy_score(y_predict, model.predict(X_predict)))\n",
    "    else:\n",
    "        scores.append(rmse(y_test, model.predict(X_test)))\n",
    "    end = time.time()\n",
    "    prediction_times.append(end - start)\n",
    "    print('\\nPrediction time: ')\n",
    "    show_time(end - start)\n",
    "    return model\n",
    "\n",
    "# Takes in a classifier, calculates the training + prediction times and accuracy score, returns a model\n",
    "def GridSearch(clf, params, X, y, X_predict, y_predict, type='classification'):\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    if(type=='classification'):\n",
    "        model = GridSearchCV(clf, params, scoring='accuracy', n_jobs=-1, cv=5).fit(X,y).best_estimator_\n",
    "    else:\n",
    "        model = GridSearchCV(clf, params, scoring='r2', n_jobs=-1, cv=5).fit(X,y).best_estimator_\n",
    "    end = time.time()\n",
    "    print('Training time: ')\n",
    "    show_time(end - start)\n",
    "    training_times.append(end - start)\n",
    "\n",
    "    # Predict\n",
    "    start = time.time()\n",
    "    if(type=='classification'):\n",
    "        scores.append(accuracy_score(y_predict, model.predict(X_predict)))\n",
    "    else:\n",
    "        scores.append(rmse(y_test, model.predict(X_test)))\n",
    "    end = time.time()\n",
    "    prediction_times.append(end - start)\n",
    "    print('Prediction time: ')\n",
    "    show_time(end - start)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in model scores and plots them on a bar graph\n",
    "def plot_metric(model_scores, score='Accuracy'):\n",
    "    # Set figure size\n",
    "    rcParams['figure.figsize'] = 7,5\n",
    "    plt.bar(model_scores['Model'], height=model_scores[score])\n",
    "    xlocs, xlabs = plt.xticks()\n",
    "    xlocs=[i for i in range(0,6)]\n",
    "    xlabs=[i for i in range(0,6)]\n",
    "    if(score != 'Prediction Times'):\n",
    "        for i, v in enumerate(model_scores[score]):\n",
    "            plt.text(xlocs[i] - 0.25, v + 0.01, str(v))\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel(score)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in training data and a model, and plots a bar graph of the model's feature importances\n",
    "def feature_importances(df, model, model_name, max_num_features=10):\n",
    "    feature_importances = pd.DataFrame(columns = ['feature', 'importance'])\n",
    "    feature_importances['feature'] = df.columns\n",
    "    feature_importances['importance'] = model.feature_importances_\n",
    "    feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "    feature_importances = feature_importances[:max_num_features]\n",
    "    # print(feature_importances)\n",
    "    plt.figure(figsize=(12, 6));\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances);\n",
    "    plt.title(model_name+' features importance:');\n",
    "\n",
    "# Takes in training data and a model, and plots a bar graph of SHAP values\n",
    "def shap_values(df, model, model_name):\n",
    "    shap_values = shap.TreeExplainer(model).shap_values(df)\n",
    "    shap_values[:5]\n",
    "    shap.summary_plot(shap_values, df.iloc[:1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 1: Classification: Classify images in the Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Explore the Fashion MNIST dataset (60000 rows, 784 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "fetch_from = '../input/fashionmnist/fashion-mnist_train.csv'\n",
    "train = pd.read_csv(fetch_from)\n",
    "\n",
    "fetch_from = '../input/fashionmnist/fashion-mnist_test.csv'\n",
    "test = pd.read_csv(fetch_from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train-test split\n",
    "X_train, y_train, X_test, y_test = train.iloc[:,1:], train['label'], test.iloc[:,1:], test['label']\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape\n",
    "# Each image is 28*28(=784) pixels, hence the 784 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some images in the dataset\n",
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "plt.figure(figsize=(9,9))\n",
    "example_images = X_train[:100]\n",
    "plot_digits(example_images.values, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_times = []\n",
    "training_times = []\n",
    "scores = []\n",
    "# training_times\n",
    "# prediction_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = Train(XGBClassifier(n_estimators=50, max_depth=5), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = Train(LGBMClassifier(n_estimators=50, max_depth=5), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Train(CatBoostClassifier(n_estimators=50, verbose=False, max_depth=6), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Fine-tuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with GridSearch\n",
    "param_grid=[{'max_depth':[5,10],\n",
    "           'n_estimators':[100],\n",
    "           'learning_rate':[0.05,0.1],\n",
    "           'colsample_bytree':[0.8,0.95]}]\n",
    "xgboost_gs = GridSearch(XGBClassifier(random_state=42), param_grid, X_train[:4000], y_train[:4000], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with GridSearch\n",
    "param_grid=[{'max_depth':[5,10],\n",
    "           'n_estimators':[100],\n",
    "           'learning_rate':[0.05,0.1],\n",
    "           'colsample_bytree':[0.8,0.95]}]\n",
    "lgb_gs = GridSearch(LGBMClassifier(random_state=42), param_grid, X_train[:4000], y_train[:4000], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# CatBoost with GridSearch\n",
    "start = time.time()\n",
    "param_grid=[{'n_estimators':[10,100],\n",
    "            'learning_rate':[0.05,0.1],\n",
    "            'rsm':[0.5,0.8]}]\n",
    "cat_gs = GridSearch(CatBoostClassifier(random_state=42, silent = True,\n",
    "                        bootstrap_type = 'Bernoulli'), param_grid, X_train[:4000], y_train[:4000], X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory be deleting dataframes no longer needed\n",
    "del [[y_train, X_test, y_test, train, test]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('XGBoost', xgboost),\n",
    "         ('LightGBM', lgb),\n",
    "         ('CatBoost', cat),\n",
    "         ('XGBoost GridSearch', xgboost_gs),\n",
    "         ('LightGBM GridSearch', lgb_gs),\n",
    "         ('CatBoost GridSearch', cat_gs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = pd.DataFrame({ 'Model': [name for name, _ in models], 'Accuracy': scores })\n",
    "model_scores.sort_values(by='Accuracy',ascending=False,inplace=True)\n",
    "plot_metric(model_scores, score='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Prediction Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = [round(time,2) for time in training_times]\n",
    "model_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Training Times': training_times })\n",
    "plot_metric(model_train_times, score='Training Times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_times = [round(time,2) for time in prediction_times]\n",
    "model_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Prediction Times': prediction_times })\n",
    "plot_metric(model_train_times, score='Prediction Times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model's prediction score only paints a partial picture of its predictions. We also want to know *why* the model is making its predictions.\n",
    "\n",
    "Here we plot the model's feature importances, SHAP values and draw an actual decision tree to get a firmer understanding of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "feature_importances(X_train, xgboost, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "feature_importances(X_train, cat, 'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "feature_importances(X_train, lgb, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference table for understanding which class names the class indexes refer to in the graphs below:\n",
    "\n",
    "| Class | Name |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "shap_values(X_train.iloc[:500,:], xgboost, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "shap_values(X_train.iloc[:500,:], lgb, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost**\n",
    "CatBoost doesn't work out of the box with shap_values() and results in the kernel crashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size for decision tree plots\n",
    "rcParams['figure.figsize'] = 80,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lightgbm.plot_tree(lgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb.plot_tree(xgboost);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost**\n",
    "\n",
    "CatBoost ships with no plotting function for its trees. If you really need to visualize CatBoost results, a work-around is proposed here: https://blog.csdn.net/l_xzmy/article/details/81532281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before moving onto the next round\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [[X_train]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 2: Regression on a medium Dataset: Predict NYC Taxi fares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Explore the NYC Taxi dataset (60,000 rows, 6 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from New York City Taxi Fare Prediction\n",
    "n = 60000\n",
    "train = pd.read_csv('../input/nyctaxi/train.csv', nrows=n)\n",
    "test = pd.read_csv('../input/nyctaxi/test.csv')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# this cell was adapted from https://www.kaggle.com/mahtieu/nyc-taxi-fare-prediction-data-expl-xgboost\n",
    "def feature_engineering(df):\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    #Drop rows with null values\n",
    "    df = df.dropna(how = 'any', axis = 'rows')\n",
    "    #Free rides, negative fares and passenger count filtering\n",
    "    df = df[df.eval('(fare_amount > 0) & (passenger_count <= 6)')]\n",
    "    # Coordinates filtering - Pickup and dropoff locations should be within the limits of NYC\n",
    "    df = df[(df.pickup_longitude >= -77) &\n",
    "                  (df.pickup_longitude <= -70) &\n",
    "                  (df.dropoff_longitude >= -77) &\n",
    "                  (df.dropoff_longitude <= 70) &\n",
    "                  (df.pickup_latitude >= 35) &\n",
    "                  (df.pickup_latitude <= 45) &\n",
    "                  (df.dropoff_latitude >= 35) &\n",
    "                  (df.dropoff_latitude <= 45)]\n",
    "\n",
    "    df.pickup_datetime = df.pickup_datetime.dt.tz_localize('UTC')\n",
    "    df.pickup_datetime = df.pickup_datetime.dt.tz_convert(tz.gettz('America/New_York'))\n",
    "\n",
    "    # Fares may change every year\n",
    "    df['year'] = df.pickup_datetime.dt.year\n",
    "\n",
    "    # Different fares during weekdays and weekends\n",
    "    df['dayofweek'] = df.pickup_datetime.dt.dayofweek\n",
    "\n",
    "    # Different fares during public holidays\n",
    "    df['dayofyear'] = df.pickup_datetime.dt.dayofyear\n",
    "\n",
    "    # Different fares in peak periods and off-peak periods\n",
    "    df['hourofday'] = df.pickup_datetime.dt.hour\n",
    "\n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "\n",
    "    # Computes the distance (in miles) between the pickup and the dropoff locations\n",
    "    df['distance'] = df.apply(\n",
    "        lambda x: distance.distance((x.pickup_latitude, x.pickup_longitude), (x.dropoff_latitude, x.dropoff_longitude)).miles,\n",
    "        axis = 1)\n",
    "\n",
    "    df = df[df.eval('(distance > 0) & (distance < 150)')]\n",
    "    fare_distance_ratio = (df.fare_amount/df.distance)\n",
    "    fare_distance_ratio.describe()\n",
    "\n",
    "    (fare_distance_ratio[fare_distance_ratio < 45]).hist()\n",
    "\n",
    "    # Drop incoherent fares\n",
    "    df = df[fare_distance_ratio < 45]\n",
    "    del fare_distance_ratio\n",
    "\n",
    "    # Coordinates of the 3 airpots of NYC\n",
    "    airports = {'jfk': [40.6441666, -73.7822222],\n",
    "                'laguardia': [40.7747222, -73.8719444],\n",
    "                'newark': [40.6897222, -74.175]}\n",
    "\n",
    "    # Computes the distance between the pickup location and the airport\n",
    "    pickup = df.apply(lambda x: distance.distance((x.pickup_latitude, x.pickup_longitude), (airports.get('jfk'))).miles, axis=1)\n",
    "    # Computes the distance between the dropoff location and the airport\n",
    "    dropoff = df.apply(lambda x: distance.distance((x.dropoff_latitude, x.dropoff_longitude), (airports.get('jfk'))).miles, axis=1)\n",
    "    # Selects the shortest distance\n",
    "    df['to_jfk'] = pd.concat((pickup, dropoff), axis=1).min(axis=1)\n",
    "\n",
    "    pickup = df.apply(lambda x: distance.distance((x.pickup_latitude, x.pickup_longitude), (airports.get('laguardia'))).miles, axis=1)\n",
    "    dropoff = df.apply(lambda x: distance.distance((x.dropoff_latitude, x.dropoff_longitude), (airports.get('laguardia'))).miles, axis=1)\n",
    "    df['to_laguardia'] = pd.concat((pickup, dropoff), axis=1).min(axis=1)\n",
    "\n",
    "    pickup = df.apply(lambda x: distance.distance((x.pickup_latitude, x.pickup_longitude), (airports.get('newark'))).miles, axis=1)\n",
    "    dropoff = df.apply(lambda x: distance.distance((x.dropoff_latitude, x.dropoff_longitude), (airports.get('newark'))).miles, axis=1)\n",
    "    df['to_newark'] = pd.concat((pickup, dropoff), axis=1).min(axis=1)\n",
    "    del pickup, dropoff\n",
    "    return df\n",
    "\n",
    "def remove_sparse(df):\n",
    "    features = [x for x in df.columns]\n",
    "    for feature in features:\n",
    "        if len(np.unique(df[feature]))<2:\n",
    "            df.drop(feature, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "train = remove_sparse(train)\n",
    "test = remove_sparse(test)\n",
    "train = feature_engineering(train)\n",
    "test = feature_engineering(test)\n",
    "y_train = train.fare_amount\n",
    "X_train = train.drop('fare_amount', axis=1)\n",
    "y_test = test.fare_amount\n",
    "X_test = test.drop('fare_amount', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_times = []\n",
    "training_times = []\n",
    "scores = []\n",
    "# training_times\n",
    "# prediction_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgboost = Train(XGBRegressor(n_estimators=50,\n",
    "                        max_depth = 9,\n",
    "                        boosting_type = 'gbdt',\n",
    "                        learning_rate = 0.05,\n",
    "                        subsample = 0.85,\n",
    "                        colsample_bytree = 0.85,\n",
    "                        reg_alpha = 1e-4,\n",
    "                        silent = True,\n",
    "                        n_jobs = -1), X_train, y_train, X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgb = Train(LGBMRegressor(n_estimators=50,\n",
    "                    max_depth = 9,\n",
    "                    boosting_type = 'gbdt',\n",
    "                    learning_rate = 0.05,\n",
    "                    subsample = 0.85,\n",
    "                    colsample_bytree = 0.85,\n",
    "                    reg_alpha = 1e-4,\n",
    "                    silent = True,\n",
    "                    n_jobs = -1), X_train, y_train, X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost\n",
    "cat = Train(CatBoostRegressor(n_estimators=50,\n",
    "                        max_depth = 9,\n",
    "                        loss_function = 'RMSE',\n",
    "                        eval_metric = 'RMSE',\n",
    "                        learning_rate = 0.05,\n",
    "                        boosting_type = 'Plain',\n",
    "                        bootstrap_type = 'Bernoulli',\n",
    "                        subsample = 0.85,\n",
    "                        silent = True), X_train, y_train, X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Fine-tuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with GridSearch\n",
    "param_grid = [{'n_estimators': [10,100],\n",
    "               'max_depth': [5, 10],\n",
    "               'colsample_bytree': [0.8, 0.9],\n",
    "               'learning_rate': [0.05, 0.1],\n",
    "               'boosting_type': ['gbdt'],\n",
    "               'reg_alpha': [1e-4]\n",
    "               }]\n",
    "xgboost_gs = GridSearch(XGBRegressor(), param_grid, X_train[:4000], y_train[:4000], X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with GridSearch\n",
    "param_grid = [{'n_estimators': [10,100],\n",
    "               'max_depth': [5, 10],\n",
    "               'colsample_bytree': [0.8, 0.9],\n",
    "               'learning_rate': [0.05, 0.01],\n",
    "               'boosting_type': ['gbdt'],\n",
    "               'reg_alpha': [1e-4]\n",
    "               }]\n",
    "lgb_gs = GridSearch(LGBMRegressor(), param_grid, X_train[:4000], y_train[:4000], X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost with GridSearch\n",
    "param_grid = [{'n_estimators': [10,100],\n",
    "               'learning_rate': [0.05, 0.01],\n",
    "               'subsample': [0.8, 0.9]\n",
    "               }]\n",
    "cat_gs = GridSearch(CatBoostRegressor(loss_function = 'RMSE',\n",
    "                        eval_metric = 'RMSE',\n",
    "                        boosting_type = 'Plain',\n",
    "                        bootstrap_type = 'Bernoulli',\n",
    "                        silent = True), param_grid, X_train[:4000], y_train[:4000], X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('XGBoost', xgboost),\n",
    "         ('LightGBM', lgb),\n",
    "         ('CatBoost', cat),\n",
    "         ('XGBoost GridSearch', xgboost_gs),\n",
    "         ('LightGBM GridSearch', lgb_gs),\n",
    "         ('CatBoost GridSearch', cat_gs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [round(score) for score in scores]\n",
    "model_scores = pd.DataFrame({ 'Model': [name for name, _ in models], 'R2': scores })\n",
    "model_scores.sort_values(by='R2',ascending=False,inplace=True)\n",
    "plot_metric(model_scores, score='R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Prediction Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = [round(time,2) for time in training_times]\n",
    "model_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Training Times': training_times })\n",
    "plot_metric(model_train_times, score='Training Times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Prediction Times': prediction_times })\n",
    "plot_metric(model_train_times, score='Prediction Times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model's prediction score only paints a partial picture of its predictions. We also want to know *why* the model is making its predictions.\n",
    "\n",
    "Here we plot the model's feature importances, SHAP values and draw an actual decision tree to get a firmer understanding of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "feature_importances(X_train, xgboost, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "feature_importances(X_train, cat, 'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "feature_importances(X_train, lgb, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "shap_values(X_train.iloc[:500,:], xgboost, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "shap_values(X_train.iloc[:500,:], lgb, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost**\n",
    "CatBoost doesn't work out of the box with shap_values() and results in the kernel crashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size for decision tree plots\n",
    "rcParams['figure.figsize'] = 80,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lightgbm.plot_tree(lgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb.plot_tree(xgboost);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost**\n",
    "\n",
    "CatBoost ships with no plotting function for its trees. If you really need to visualize CatBoost results, a work-around is proposed here: https://blog.csdn.net/l_xzmy/article/details/81532281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before moving onto the next round\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round 3: Regression on a massive Dataset: Predict NYC Taxi fares (2 million rows, 7 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NYC Taxi dataset (2 million rows, 6 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using the same dataset as above, but this time instead of training on 60,000 rows, we're going to train the models on 2 million rows and see if they're up to the challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from New York City Taxi Fare Prediction\n",
    "# allocate 1000 rows for test set, the rest for training set\n",
    "n=2000000\n",
    "train = pd.read_csv('../input/nyctaxi/train_20mil.csv', nrows=n)\n",
    "test = pd.read_csv('../input/nyctaxi/train_20mil.csv', skiprows=n)\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_times = []\n",
    "training_times = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "train = remove_sparse(train)\n",
    "test = remove_sparse(test)\n",
    "train = feature_engineering(train)\n",
    "test = feature_engineering(test)\n",
    "y_train = train.fare_amount\n",
    "X_train = train.drop('fare_amount', axis=1)\n",
    "y_test = test.fare_amount\n",
    "X_test = test.drop('fare_amount', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgboost = Train(XGBRegressor(n_estimators=3,\n",
    "                        max_depth = 9,\n",
    "                        boosting_type = 'gbdt',\n",
    "                        learning_rate = 0.05,\n",
    "                        subsample = 0.85,\n",
    "                        colsample_bytree = 0.8,\n",
    "                        reg_alpha = 1e-4,\n",
    "                        silent = True,\n",
    "                        n_jobs = -1), X_train, y_train, X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgb = Train(LGBMRegressor(n_estimators=3,\n",
    "                    max_depth = 9,\n",
    "                    boosting_type = 'gbdt',\n",
    "                    learning_rate = 0.05,\n",
    "                    subsample = 0.85,\n",
    "                    colsample_bytree = 0.8,\n",
    "                    reg_alpha = 1e-4,\n",
    "                    silent = True,\n",
    "                    n_jobs = -1), X_train, y_train, X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost\n",
    "cat = Train(CatBoostRegressor(n_estimators=3,\n",
    "                        max_depth = 9,\n",
    "                        loss_function = 'RMSE',\n",
    "                        eval_metric = 'RMSE',\n",
    "                        boosting_type = 'Plain',\n",
    "                        bootstrap_type = 'Bernoulli',\n",
    "                        learning_rate = 0.05,\n",
    "                        subsample = 0.85,\n",
    "                        silent = True), X_train, y_train, X_test, y_test, type='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('XGBoost', xgboost),\n",
    "         ('LightGBM', lgb),\n",
    "         ('CatBoost', cat)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R2 Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [round(score) for score in scores]\n",
    "model_scores = pd.DataFrame({ 'Model': [name for name, _ in models], 'R2': scores })\n",
    "model_scores.sort_values(by='R2',ascending=False,inplace=True)\n",
    "plot_metric(model_scores, score='R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Prediction Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = [round(time,2) for time in training_times]\n",
    "model_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Training Times': training_times })\n",
    "plot_metric(model_train_times, score='Training Times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_times = pd.DataFrame({ 'Model': [name for name, _ in models], 'Prediction Times': prediction_times })\n",
    "plot_metric(model_train_times, score='Prediction Times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "feature_importances(X_train, xgboost, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "feature_importances(X_train, cat, 'CatBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "feature_importances(X_train, lgb, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "shap_values(X_train.iloc[:500,:], xgboost, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "shap_values(X_train.iloc[:500,:], lgb, 'LightGBM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost**\n",
    "CatBoost doesn't work out of the box with shap_values() and results in the kernel crashing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size for decision tree plots\n",
    "rcParams['figure.figsize'] = 80,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lightgbm.plot_tree(lgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb.plot_tree(xgboost);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CatBoost**\n",
    "\n",
    "CatBoost ships with no plotting function for its trees. If you really need to visualize CatBoost results, a work-around is proposed here: https://blog.csdn.net/l_xzmy/article/details/81532281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before moving onto the next round\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis\n",
    "\n",
    "I hope you find this analysis useful! I encourage you to fork this kernel, and play with the code.\n",
    "\n",
    "A detailed analysis of the results obtained in this kernel can be found on my blog at: https://lavanya.ai/2019/06/27/battle-of-the-boosting-algorithms/\n",
    "\n",
    "If you like this kernel, please give it an upvote. Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "207px",
    "width": "186px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
